  # Equations of the SEIR model
def SEIR(INPUT, t, beta, gamma, delta, b, m):
    S, E, I, R = INPUT  
    N = S + E + I + R
    dS_dt =   - (beta * (I / N) * S) + b*N - m*S 
    dE_dt = (beta * (I / N) * S) - delta*E - m*E
    dI_dt =  delta*E - (gamma * I) - m*I
    dR_dt =  (gamma * I) - m*R
        
    return dS_dt, dE_dt, dI_dt, dR_dt
    
    
    # Definition the calibration function
def calibration_parameters(city_idx, ds, latent_period, Pop0, t_range, plot = False): 
    MSEs = []    #create lists for storage of the results of the calibration
    I_hats = []
    R0_list = []
    d_list = []
    l_list = []
    params = []
    t_city = t[city_idx] #retrieve the time range for the province considered
    
    for R0 in np.arange(1,3,0.1): #here I considered only a few values for R0
                                  #because the main aim of this function is to calibrate
                                  #gamma and delta; R0 will be "refined" later on                   
        for d in ds: 
            beta = R0*1/d 
            gamma = 1/d

            for l in latent_period:
                delta = 1/l
                                    #iterate the SEIR model over time:
                RES = odeint(SEIR, Population_t0.iloc[city_idx,:], t_city, args = (beta, gamma, delta, b, m))
                
                S, E, I_hat, R = RES.T  #store the results
                I_daily = (I_hat[1:] + R[1:]) - (I_hat[:-1] + R[:-1]) #calculate daily variation
                I_hats.append(I_daily)
                true_data = provinces.iloc[:len(I_daily), city_idx]

                MSE = np.sqrt(sum((I_daily - true_data)**2)/len(true_data)) #calculate the error between true and predicted data
                MSEs.append(MSE)
                R0_list.append(R0)
                d_list.append(d)
                l_list.append(l)
                params.append([R0, d, l])
    
    cal = pd.DataFrame({'params':params,'R0': R0_list, 'd': d_list, 'l': l_list, 'MSE': MSEs, 'I_hat': I_hats})
    best_model = cal.iloc[cal['MSE'].idxmin()]
    return cal, best_model # returns a DataFrame containing all the results
                           # and a DataFrame containing the parameters of the best model
                           
                           


d_cities = [] 
l_cities = []

  # Run the calibration function for each province
for i in range(ncities):  
    t_city = np.arange(1, ndays[i] + 1,1)
    true_data_city = provinces.iloc[:ndays[i],1]
    Pop0_city = db.iloc[i]['S0':'R0']
    cum_cases = np.cumsum(true_data_city)

    cal, best_model = calibration_parameters(i, np.arange(3,14,1), np.arange(3,14,1), Pop0_city, t_city)
    print('BEST MODEL for {}: R0: {}, d: {}, l: {} - MSE:{}'.format(city_names[i], round(best_model['R0'], 2), 
                                                                    best_model['d'],best_model['l'], 
                                                                    round(best_model['MSE'],2)))
    d_cities.append(best_model['d'])
    l_cities.append(best_model['l'])
    
    
    
  # Preparing the second tuning function:
  # calibration_R0 is used to calibrate the basic reproductive number, R0,
  # using the previously calibrated values for gamma = 1/d and delta = 1/l

def calibration_R0(city_idx, d_city, l_city, time_range, plot = False):
    MSEs = []  
    I_hats = []
    R0s = np.arange(1, 3, 0.001) #defining an appropriate range for the calibration of R0
    
    for R0 in R0s:
        beta = (R0*(1/d_city[city_idx]))
        delta = 1/l_city[city_idx]  # calculate delta and gamma with the optimlal
        gamma = 1/d_city[city_idx]  # parameters defined with the previous calibration
        
        RES = odeint(SEIR, Population_t0.iloc[city_idx,:], time_range, args = (beta, gamma, delta, b, m))
        S, E, I_hat, R = RES.T
        I_daily = (I_hat[1:]+R[1:])-(I_hat[:-1]+R[:-1])
        I_hats.append(I_daily)
              
        column_data = provinces.iloc[:,city_idx] # specifying the true data for the province
        true_data = column_data[:len(I_daily)]
        MSE = np.sqrt(sum((I_daily-true_data)**2)/len(true_data))
        MSEs.append(MSE)
    cal = pd.DataFrame({'R0': R0s, 'MSE': MSEs, 'I_hat': I_hats})
    best_model = cal.loc[cal['MSE'].idxmin()]
    print('BEST MODEL for {}: RO: {}, MSE:{}'.format(city_names[city_idx], 
                                                 round(best_model['R0'], 2), round(best_model['MSE'],2)))
    
    if print:  #if print = True, the function returns the plots
        title = city_names[city_idx]
        fig, axs = plt.subplots(1, 3, facecolor = 'w', figsize=(10,3))
        fig.suptitle(f'{title}')
        axs[0].set_title('Best Model for {}: \n RO: {}, MSE:{}'.format(city_names[city_idx],
                                                            round(best_model['R0'], 2), round(best_model['MSE'],2)))
        axs[0].scatter(time_range, column_data[:len(time_range)], color = 'slateblue', label = 'I_data')
        axs[0].plot(time_range[:-1] ,best_model['I_hat'], alpha=1, lw=2, label='I_daily', color = 'darkred')
        axs[0].set_xlabel('New cases')
        axs[0].set_ylabel('Time')
        
        axs[1].set_title('Models Tested')
        axs[1].plot(time_range[:-1], I_hats[0], alpha=0.5,  color = 'darkgray', lw=1, label ='models tested' )
        for R0, I in zip(R0s, I_hats[1:]):
            if R0 == best_model['R0']:
                axs[1].plot(time_range[:-1], I, c = 'darkred', lw=5, label = 'best model')
            else:
                axs[1].plot(time_range[:-1], I, alpha=0.05,  color = 'darkgray', lw=1)
        axs[1].set_xlabel('Infected')
        axs[1].set_ylabel('Time')
        
        axs[2].set_title('MSEs')
        axs[2].scatter(cal['R0'], cal['MSE'], color = 'darkred' , s =1 )
        axs[2].scatter(best_model['R0'], best_model['MSE'], color = 'lightsalmon' , s =50, label='min MSE' )
        axs[2].axvline(x = best_model['R0'], ymin=0, ymax=2, c = 'slateblue')
        axs[2].set_xlabel('R0s')
        axs[2].set_ylabel('MSEs')

        for i in range(3):
            axs[i].legend(fancybox = True)
            for spine in ('top', 'right'): 
                axs[i].spines[spine].set_visible(False)

        fig.tight_layout()
        plt.show()
        
        
for i in range(ncities):
    t_city = np.arange(1, ndays[i] + 1,1)
    true_data_city = provinces.iloc[:ndays[i],1]
    Pop0_city = db.iloc[i]['S0':'R0']
    cum_cases = np.cumsum(true_data_city)

    calibration_R0(i, d_cities, l_cities, t_city, plot = True)
    
'''
 If plot = True, the function calibration_R0 produces 3 plots for each province tested:
    1.) The scatter plot of true new daily cases the predicted curve of daily variation of infected
    2.) The predicted curve of daily variation of infected for every value of R0, with the 
        red one corresponding to the best model, i.e. the one with the optimal R0
    3.) The plot of all the MSEs calculated for the different values of R0, 
        with a line indicating the best value of R0 (i.e. R0 corresponding to the lowest MSE)
'''


  # Alternative ways of performing calibration
# Here, applied only to the province of Pavia, just for the sake of proving that
#they lead to the same results

# 1. MAXIMUM LIKELIHOOD ESTIMATION

def compute_likelihood(data, data_hat):
    lkh = 1
    for i in range(len(data)-1):
        lkh = lkh * poisson.pmf(data[i],data_hat[i]) #poisson likelihood
        if str(lkh) == 'nan':
            lkh = 0
            break
    return lkh

def compute_loglikelihood(data, data_hat):
    llkh = 0
    for i in range(len(data)-1):
        llkh = llkh + np.log(poisson.pmf(data[i],data_hat[i])) #poisson likelihood to observe the data with a model
    return llkh

# Definintion of the calibration function based on the maximization
# of the likelihood of observing the data at hand

lkhs = []
llkhs = []
I_hats = []
def MLE_calibration(city_idx, R0s, d_city, l_city, Pop0_city, t_city):
    for R0 in R0s:  
            # Only calibrate R0; gamma and delta are fixed using the optimal d and l calculated before
        beta = R0*1/d_city 
        gamma = 1/d_city
        delta = 1/l_city

        RES = odeint(SEIR, Pop0_city, t_city, args=(beta, gamma, delta, b, m))
        S, E, I_hat, R = RES.T
        I_daily = (I_hat[1:]+R[1:])-(I_hat[:-1]+R[:-1])
        I_hats.append(I_daily)
        true_data = provinces.iloc[:len(I_daily),city_idx]
        
        #compute likelihood and loglikelihood 
        lkhs.append(compute_likelihood(true_data,I_daily))
        llkhs.append(compute_loglikelihood(true_data, I_daily))

    return pd.DataFrame({'R0': R0s, 'Likelihood': lkhs, 'LogLikelihood': llkhs, 'I_hat': I_hats})
    

# MLE results for the calibration of R0 for the province of Pavia
MLE_PV = MLE_calibration(1, R0s = np.arange(1, 3, 0.001),
                         d_city = d_cities[1],
                         l_city = l_cities[1], 
                         Pop0_city = Pop0_PV, 
                         t_city = t_PV)
                         
# Select the best model, i.e. the model with the highest Likelihood:
best_model_l = MLE_PV.loc[MLE_PV['Likelihood'].idxmax()]
print('BEST MODEL FOR PAVIA: R0: {}, Likelihood:{}'.format(round(best_model_l['R0'], 2), 
                                                     round(best_model_l['Likelihood'],2)))
                                                     
                                                     
# Equivalently, select the best model can be defined as the model with the highest LogLikelihood:
best_model_log = MLE_PV.loc[MLE_PV['LogLikelihood'].idxmax()]
print('BEST MODEL FOR PAVIA: R0: {}, LogLikelihood:{}'.format(round(best_model_log['R0'], 2), 
                                                     round(best_model_log['LogLikelihood'],2)))
                                                     

# Plot of the results of Maximum Likelihood Estimation for Pavia
true_data_PV = provinces.iloc[:len(t_PV),1]
R0s = np.arange(1, 3, 0.001)

fig, axs = plt.subplots(2,2, facecolor='w', figsize=(9,4))
axs[0,0].set_title('Best Model for PAVIA: R0: {}, Likelihood:{}'.format(round(best_model_l['R0'], 2), 
                                                                        round(best_model_l['Likelihood'],2)))
axs[0,0].scatter(t_PV, true_data_PV, color = 'slateblue', label = 'I_data')
axs[0,0].plot(t_PV[:-1] ,best_model_l['I_hat'], alpha=1, lw=2, label='I_hat', color = 'darkred')

axs[0,1].set_title('Models Tested')
axs[0,1].plot(t_PV[:-1], best_model_l['I_hat'], alpha=0.5,  color = 'k', lw=1, label ='models tested' )
for R0, I in zip(R0s, I_hats[1:]):
    if R0 == best_model_l['R0']:
        axs[0,1].plot(t_PV[:-1], I, c = 'darkred', lw=5, label = 'best model')
    else:
        axs[0,1].plot(t_PV[:-1], I, alpha=0.05,  color = 'darkgrey', lw=1)

axs[1,0].set_title('Likelihoods')
axs[1,0].scatter(MLE_PV['R0'], MLE_PV['Likelihood'], color = 'k' , s =2 )
axs[1,0].scatter(best_model_l['R0'], best_model_l['Likelihood'], color = 'darkred' , s =50, label='max lkh' )
axs[1,0].axvline(x = best_model_l['R0'], ymin=0, ymax=2, c= 'slateblue')
axs[1,0].set_ylim(-best_model_l['Likelihood']/2 , best_model_l['Likelihood']*1.2)
axs[1,0].set_xlim(best_model_l['R0']*(1-0.01) , best_model_l['R0']*1.01)

axs[1,0].set_xlabel('R0s')
axs[1,0].set_ylabel('Likelihood')

axs[1,1].set_title('LogLikelihoods')
axs[1,1].scatter(MLE_PV['R0'], MLE_PV['LogLikelihood'], color = 'k' , s =2 )
axs[1,1].scatter(best_model_log['R0'], best_model_log['LogLikelihood'], color = 'darkred' , s =50, label='max log-lkh' )
axs[1,1].axvline(x = best_model_log['R0'], ymin=0, ymax=2, c= 'slateblue')

axs[1,1].set_xlabel('R0s')
axs[1,1].set_ylabel('Likelihood')

for i in range(2):
    for j in range(2):
        axs[i,j].legend(fancybox = True)
        for spine in ('top', 'right'): 
            axs[i,j].spines[spine].set_visible(False)
        
fig.tight_layout()
fig.show()           




# 2. Optimization with Monte Carlo Markov Chain

std = 0.01
iterations = 4000

def metropolis_hastings(iterations, beta_start, y0, t_city, gamma, delta, std, true_data):
    
    RES = odeint(SEIR, y0, t_city, args=(beta_start, gamma, delta, b, m))
    S, E, I_hat, R = RES.T
    I_daily = (I_hat[1:] + R[1:]) - (I_hat[:-1] + R[:-1])
    llkh_start = compute_loglikelihood(true_data, I_daily)
    
    BETAS = [beta_start]
    llkh = [llkh_start]
    
    for ITER in range(iterations):
    
        beta_current = BETAS[-1]
        llkh_current = llkh[-1]

        beta_new = np.random.normal(beta_current, std, size = 1) # Sampling the candidate value for beta
        RES = odeint(SEIR, y0, t_city, args=(beta_new, gamma, delta, b, m)) # Integrating the SEIR equations
        I_hat, R = RES.T[2:]

        I_daily = (I_hat[1:]+R[1:])-(I_hat[:-1]+R[:-1])
        llkh_new = compute_loglikelihood(true_data, I_daily)

        alpha  = np.e**(llkh_new - llkh_current) # acceptance ratio that will be used 
                                                 # to decide whether to accept or reject the candidate beta

        if alpha <= np.random.uniform(0,1): 
            BETAS.append(beta_current)
            llkh.append(llkh_current)

        else: 
            BETAS.append(beta_new)
            llkh.append(llkh_new)
      
    # Discard the first 400 iterations. 
    BETAS = BETAS[400:] 
    llkh = llkh[400:]
    
    return np.array(BETAS).flatten(), np.array(llkh).flatten()
    
    
# Trying different initial values for beta
BETAS_1, llkh_1 = metropolis_hastings(iterations, 0.2, Pop0_PV, t_PV, gamma_PV, delta_PV, std, true_data_PV)  
BETAS_2, llkh_2 = metropolis_hastings(iterations, 0.3, Pop0_PV, t_PV, gamma_PV, delta_PV, std, true_data_PV) 
BETAS_3, llkh_3= metropolis_hastings(iterations, 0.4, Pop0_PV, t_PV, gamma_PV, delta_PV, std, true_data_PV)  

B = [BETAS_1, BETAS_2, BETAS_3]
LLK = [llkh_1, llkh_2, llkh_3]


#  Plotting the results of Metropolis Hastings algorithm
fig, axs = plt.subplots(1,4, facecolor='w', figsize=(10,3))

axs[0].set_title('Chains')
axs[1].set_title('Log-likelihoods')
axs[2].set_title('Posterior')
axs[3].set_title('Posterior')

colors = ['deeppink', 'dodgerblue', 'peachpuff']
for i in range(3):
    BETAS = B[i]
    llk = LLK[i]
    
    axs[0].plot(range(len(BETAS)), BETAS, label = 'chain {}'.format(i+1), alpha = 0.7, c = colors[i])
    axs[1].plot(range(len(llk)), llk, alpha = 0.7, c = colors[i])
    axs[2].hist(BETAS, bins = 20, alpha = 0.6, color = colors[i])

axs[0].set_xlabel('Iteration')
axs[0].set_ylabel('Beta')

axs[1].set_xlabel('Iteration')
axs[1].set_ylabel('Log-likelihood')

axs[2].set_xlabel('Beta')
axs[2].set_ylabel('Count')

axs[3].boxplot(B, showmeans=True)
plt.setp(axs[3], xticklabels=['chain 1', 'chain 2', 'chain 3']);

axs[0].legend(fancybox = True)

for i in range(4):
    for spine in ('top', 'right'): 
        axs[i].spines[spine].set_visible(False)
        
fig.tight_layout()
fig.show()
